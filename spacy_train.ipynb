{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cc4633",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.data_models import Document\n",
    "from openai_api import *\n",
    "\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "from openai import AzureOpenAI\n",
    "import azure.keyvault.secrets as azk\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import spacy\n",
    "from spacy.tokens import DocBin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0eed8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def clean(text):\n",
    "    text = text.strip().replace(\"\\n\", \" \").replace(\"\\r\", \"\").replace(\"\\t\", \" \")\n",
    "    text = re.sub('\\\\s+', ' ', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "resumes = []\n",
    "for file in tqdm(os.listdir(f'{os.getcwd()}/DATA/data_en')):\n",
    "    with open(f'{os.getcwd()}/DATA/data_en/{file}','r') as f:\n",
    "        contents = clean(f.read())\n",
    "        resumes.append(contents)\n",
    "\n",
    "with open('resumes.json','x') as j:\n",
    "    j.write(json.dumps(resumes,indent=4,ensure_ascii=False))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39761c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [34:43<00:00,  4.17s/it]  \n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "with open('resumes.json','r') as f:\n",
    "    resumes = json.loads(f.read())\n",
    "\n",
    "    \n",
    "labeled_resumes = []\n",
    "\n",
    "for i in tqdm(range(len(resumes[:500]))):\n",
    "    text = resumes[i]\n",
    "    api_labels = await send_request(text)\n",
    "\n",
    "    labels = []\n",
    "    for l in api_labels or []:\n",
    "        label = l['type']\n",
    "        content = l['text']\n",
    "        start = text.find(content)\n",
    "        if start == -1:\n",
    "            continue\n",
    "        end = start + len(content)\n",
    "        labels.append(Label(start,end,label,content).to_dict())\n",
    "\n",
    "    doc = Document(i,text,labels).to_dict()\n",
    "    labeled_resumes.append(doc)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80582e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cleaned_resumes.json','r') as j:\n",
    "    labeled_resumes = json.loads(j.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c22195d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_resumes = []\n",
    "\n",
    "delete = ['appraisal','view','account','events','written','verbal','network','application','concepts','develop','copy','mechanical','structural','safety','conflict','culture','organizational','phone','taxes','automation','profit','skills','pc skills','research','practices','procedures','experience','skill','retail','B.S.','Healthcare','Network','policies','financial','automotive','oral','interpersonal','website','workbench','video','book','medallion','budget','agency','automate','client','clients','plan','meetings','proposals','strategy']\n",
    "deletewords = ['company name','community college','20','college','major']\n",
    "\n",
    "for r in labeled_resumes:\n",
    "  name, text, labels = r['id'],r['content'], r['labels']\n",
    "  clean_labels = []\n",
    "  starts = []\n",
    "\n",
    "  for l in labels:\n",
    "    start, end, label, value = l[\"start\"], l[\"end\"], l[\"label\"], l[\"value\"]\n",
    "    \n",
    "    if (start in starts) | (value.lower() in delete) | (len(value) >= 40):\n",
    "        continue\n",
    "\n",
    "    starts.append(start)\n",
    "    clean_labels.append(l)\n",
    "\n",
    "  cleaned_resumes.append(Document(name,text,clean_labels).to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd67ef4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cleaned_resumes.json','w') as j:\n",
    "  j.write(json.dumps(cleaned_resumes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5955195",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(cleaned_resumes, test_size=0.2, random_state=777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85a1a989",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank('en')\n",
    "doc_bin = DocBin()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b2a5888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "425 27698 411\n"
     ]
    }
   ],
   "source": [
    "def has_overlap(span1, span2):\n",
    "    return not (span1.end <= span2.start or span2.end <= span1.start)\n",
    "\n",
    "empty_count, ent_count, overlap_count = 0, 0, 0\n",
    "\n",
    "for r in train:\n",
    "    debug_id, text, labels = r[\"id\"],r[\"content\"],r[\"labels\"]\n",
    "    doc = nlp(text)\n",
    "    ents = []\n",
    "    for l in labels:\n",
    "        start, end, label, value = l[\"start\"], l[\"end\"], l[\"label\"], l[\"value\"]\n",
    "        span = doc.char_span(start_idx=start, end_idx=end, label=label)\n",
    "        if span is None:\n",
    "            empty_count +=1\n",
    "            continue\n",
    "        if any(has_overlap(span, existing_span) for existing_span in ents):\n",
    "            overlap_count +=1\n",
    "            continue\n",
    "        \n",
    "        ents.append(span)\n",
    "    ent_count += len(ents)\n",
    "    doc.ents = ents \n",
    "    doc_bin.add(doc) \n",
    "print(empty_count, ent_count, overlap_count)\n",
    "doc_bin.to_disk(\"train.spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8cb9bbf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118 7932\n"
     ]
    }
   ],
   "source": [
    "empty_count, ent_count, overlap_count = 0, 0, 0\n",
    "\n",
    "for r in test:\n",
    "    debug_id, text, labels = r[\"id\"],r[\"content\"],r[\"labels\"]\n",
    "    doc = nlp(text)\n",
    "    ents = []\n",
    "    for l in labels:\n",
    "        start, end, label, value = l[\"start\"], l[\"end\"], l[\"label\"], l[\"value\"]\n",
    "        span = doc.char_span(start, end, label=label)\n",
    "        if span is None:\n",
    "            empty_count += 1\n",
    "            continue\n",
    "        if any(has_overlap(span, existing_span) for existing_span in ents):\n",
    "            overlap_count += 1\n",
    "            continue\n",
    "        \n",
    "        ents.append(span)\n",
    "    ent_count += len(ents)\n",
    "    doc.ents = ents \n",
    "    doc_bin.add(doc) \n",
    "print(empty_count, ent_count)\n",
    "doc_bin.to_disk(\"test.spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "54079123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;3m⚠ To generate a more effective transformer-based config (GPU-only),\n",
      "install the spacy-transformers package and re-run this command. The config\n",
      "generated now does not use transformers.\u001b[0m\n",
      "\u001b[38;5;4mℹ Generated config template specific for your use case\u001b[0m\n",
      "- Language: en\n",
      "- Pipeline: ner\n",
      "- Optimize for: efficiency\n",
      "- Hardware: CPU\n",
      "- Transformer: None\n",
      "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\n",
      "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
      "config.cfg\n",
      "You can now add your data and train your pipeline:\n",
      "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
     ]
    }
   ],
   "source": [
    "! python -m spacy init config config.cfg --lang en --pipeline ner --optimize efficiency  --force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "56aa4fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\n",
      "============================ Data file validation ============================\u001b[0m\n",
      "\u001b[38;5;2m✔ Pipeline can be initialized with data\u001b[0m\n",
      "\u001b[38;5;2m✔ Corpus is loadable\u001b[0m\n",
      "\u001b[1m\n",
      "=============================== Training stats ===============================\u001b[0m\n",
      "Language: en\n",
      "Training pipeline: tok2vec, ner\n",
      "1964 training docs\n",
      "2455 evaluation docs\n",
      "\u001b[38;5;3m⚠ 1953 training examples also in evaluation data\u001b[0m\n",
      "\u001b[38;5;3m⚠ Low number of examples to train a new pipeline (1964)\u001b[0m\n",
      "\u001b[1m\n",
      "============================== Vocab & Vectors ==============================\u001b[0m\n",
      "\u001b[38;5;4mℹ 1866208 total word(s) in the data (52346 unique)\u001b[0m\n",
      "\u001b[38;5;4mℹ No word vectors present in the package\u001b[0m\n",
      "\u001b[1m\n",
      "========================== Named Entity Recognition ==========================\u001b[0m\n",
      "\u001b[38;5;4mℹ 2 label(s)\u001b[0m\n",
      "0 missing value(s) (tokens with '-' label)\n",
      "\u001b[38;5;2m✔ Good amount of examples for all labels\u001b[0m\n",
      "\u001b[38;5;2m✔ Examples without occurrences available for all labels\u001b[0m\n",
      "\u001b[38;5;2m✔ No entities consisting of or starting/ending with whitespace\u001b[0m\n",
      "\u001b[38;5;2m✔ No entities crossing sentence boundaries\u001b[0m\n",
      "\u001b[1m\n",
      "================================== Summary ==================================\u001b[0m\n",
      "\u001b[38;5;2m✔ 6 checks passed\u001b[0m\n",
      "\u001b[38;5;3m⚠ 2 warnings\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy debug data config.cfg --paths.train ./train.spacy --paths.dev ./test.spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9508434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Saving to output directory: .\u001b[0m\n",
      "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'ner']\u001b[0m\n",
      "\u001b[38;5;4mℹ Initial learn rate: 0.001\u001b[0m\n",
      "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  ------------  --------  ------  ------  ------  ------\n",
      "  0       0          0.00    375.90    0.40    0.25    0.95    0.00\n",
      "  0     200       2090.17  12718.38    1.08   25.45    0.55    0.01\n",
      "  0     400        156.17   5252.85    1.64   32.02    0.84    0.02\n",
      "  0     600        185.26   5615.28    0.01   28.57    0.01    0.00\n",
      "  0     800        184.45   5702.18    0.01   40.00    0.01    0.00\n",
      "  0    1000        203.40   5350.71    0.66   40.00    0.33    0.01\n",
      "  0    1200        281.49   5122.71    1.56   31.02    0.80    0.02\n",
      "  0    1400       4257.51   5936.71    4.79   40.53    2.55    0.05\n",
      "  0    1600        307.22   5240.45    2.26   41.95    1.16    0.02\n",
      "  0    1800        222.25   5831.24    0.08   42.86    0.04    0.00\n",
      "  1    2000        921.70   5106.45    0.03   45.45    0.01    0.00\n",
      "  1    2200        326.43   5360.77    0.11   68.97    0.06    0.00\n",
      "  1    2400        651.58   5254.42    6.25   27.70    3.52    0.06\n",
      "  1    2600        391.20   4756.62    0.59   46.67    0.29    0.01\n",
      "  1    2800        323.34   5388.64    1.83   52.71    0.93    0.02\n",
      "  1    3000        895.84   5037.38    5.10   36.43    2.74    0.05\n",
      "  1    3200      12824.84   6280.39    0.32   59.18    0.16    0.00\n",
      "  1    3400       2497.74   5516.58    0.03   75.00    0.02    0.00\n",
      "  1    3600       1259.94   5869.63    0.17   56.36    0.09    0.00\n",
      "  1    3800        373.84   5821.91    6.16   43.51    3.31    0.06\n",
      "  2    4000       1091.11   5217.67   10.22   22.30    6.63    0.10\n",
      "  2    4200        497.85   5228.90    0.82   45.96    0.42    0.01\n",
      "  2    4400      10420.07   5880.77    3.62   38.03    1.90    0.04\n",
      "  2    4600      25362.02   5567.11    7.39   39.28    4.08    0.07\n",
      "  2    4800      29746.90   5656.17   11.69   36.23    6.97    0.12\n",
      "  2    5000      24210.27   5902.90    3.37   52.86    1.74    0.03\n",
      "  2    5200        520.68   5308.57    0.95   49.71    0.48    0.01\n",
      "  2    5400      12953.48   5956.38    5.40   45.81    2.87    0.05\n",
      "  2    5600        366.58   4913.01    0.67   40.88    0.34    0.01\n",
      "  2    5800        468.88   5463.88    0.73   43.52    0.37    0.01\n",
      "  3    6000        315.82   4890.19    7.20   34.69    4.02    0.07\n",
      "  3    6200       2364.07   5802.33    3.50   43.56    1.82    0.03\n",
      "  3    6400        320.67   4865.34    0.13   66.67    0.07    0.00\n",
      "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
      "model-last\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy train config.cfg --output ./ --paths.train ./train.spacy --paths.dev ./test.spacy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ner-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
